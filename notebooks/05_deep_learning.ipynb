{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 4: Deep Learning - Multi-Layer Perceptron (MLP)\n",
    "\n",
    "**Objective**: Apply neural networks to credit rating classification and compare with classical ML models.\n",
    "\n",
    "**Questions to answer**:\n",
    "1. Can deep learning outperform Random Forest (79.68%)?\n",
    "2. What is the impact of architecture depth (Simple vs Improved)?\n",
    "3. How does training converge (learning curves)?\n",
    "4. Is deep learning worth the additional complexity for this dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, callbacks\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(f'TensorFlow version: {tf.__version__}')\n",
    "print(f'Keras version: {keras.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('../data/processed/merged_dataset_labels.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and labels\n",
    "X = df.drop(['Country', 'Year', 'Credit_Rating_Label'], axis=1)\n",
    "y = df['Credit_Rating_Label']\n",
    "\n",
    "feature_names = X.columns.tolist()\n",
    "\n",
    "print(f'Features: {feature_names}')\n",
    "print(f'Number of observations: {len(X)}')\n",
    "print(f'Number of classes: {y.nunique()}')\n",
    "print(f'\\nClass distribution:')\n",
    "print(y.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "n_classes = len(le.classes_)\n",
    "\n",
    "print(f'Label encoding: {n_classes} classes')\n",
    "print(f'Mapping: {dict(zip(le.classes_[:5], range(5)))}...')\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(f'\\nFeatures normalized:')\n",
    "print(f'  Mean: {X_scaled.mean(axis=0).round(2)}')\n",
    "print(f'  Std: {X_scaled.std(axis=0).round(2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data: 60% train, 20% validation, 20% test\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_scaled, y_encoded, test_size=0.4, random_state=42\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "print(f'Data split:')\n",
    "print(f'  Train: {len(X_train)} ({len(X_train)/len(X)*100:.1f}%)')\n",
    "print(f'  Validation: {len(X_val)} ({len(X_val)/len(X)*100:.1f}%)')\n",
    "print(f'  Test: {len(X_test)} ({len(X_test)/len(X)*100:.1f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MLP Simple (Baseline)\n",
    "\n",
    "**Architecture**:\n",
    "- Input: 8 features\n",
    "- Hidden Layer: 64 neurons (ReLU)\n",
    "- Dropout: 0.3\n",
    "- Output: 20 classes (Softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build MLP Simple\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model_simple = keras.Sequential([\n",
    "    layers.Input(shape=(X.shape[1],)),\n",
    "    layers.Dense(64, activation='relu', name='hidden_layer'),\n",
    "    layers.Dropout(0.3, name='dropout'),\n",
    "    layers.Dense(n_classes, activation='softmax', name='output_layer')\n",
    "], name='MLP_Simple')\n",
    "\n",
    "# Compile\n",
    "model_simple.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print('MLP Simple Architecture:')\n",
    "model_simple.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train MLP Simple\n",
    "early_stop = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=20,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "history_simple = model_simple.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate MLP Simple\n",
    "y_pred_simple = np.argmax(model_simple.predict(X_test, verbose=0), axis=1)\n",
    "\n",
    "acc_simple = accuracy_score(y_test, y_pred_simple)\n",
    "f1_simple = f1_score(y_test, y_pred_simple, average='weighted', zero_division=0)\n",
    "\n",
    "print(f'MLP Simple - Test Results:')\n",
    "print(f'  Accuracy: {acc_simple:.4f} ({acc_simple*100:.2f}%)')\n",
    "print(f'  F1-Weighted: {f1_simple:.4f}')\n",
    "print(f'  Epochs trained: {len(history_simple.history[\"loss\"])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning curves for MLP Simple\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history_simple.history['loss'], label='Train Loss', linewidth=2)\n",
    "axes[0].plot(history_simple.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('MLP Simple - Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(history_simple.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
    "axes[1].plot(history_simple.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('MLP Simple - Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. MLP Improved\n",
    "\n",
    "**Architecture**:\n",
    "- Input: 8 features\n",
    "- Hidden Layer 1: 128 neurons (ReLU) + BatchNorm + Dropout(0.3)\n",
    "- Hidden Layer 2: 64 neurons (ReLU) + BatchNorm + Dropout(0.3)\n",
    "- Hidden Layer 3: 32 neurons (ReLU) + Dropout(0.2)\n",
    "- Output: 20 classes (Softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build MLP Improved\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model_improved = keras.Sequential([\n",
    "    layers.Input(shape=(X.shape[1],)),\n",
    "    \n",
    "    # Layer 1\n",
    "    layers.Dense(128, activation='relu', name='hidden_layer_1'),\n",
    "    layers.BatchNormalization(name='batch_norm_1'),\n",
    "    layers.Dropout(0.3, name='dropout_1'),\n",
    "    \n",
    "    # Layer 2\n",
    "    layers.Dense(64, activation='relu', name='hidden_layer_2'),\n",
    "    layers.BatchNormalization(name='batch_norm_2'),\n",
    "    layers.Dropout(0.3, name='dropout_2'),\n",
    "    \n",
    "    # Layer 3\n",
    "    layers.Dense(32, activation='relu', name='hidden_layer_3'),\n",
    "    layers.Dropout(0.2, name='dropout_3'),\n",
    "    \n",
    "    # Output\n",
    "    layers.Dense(n_classes, activation='softmax', name='output_layer')\n",
    "], name='MLP_Improved')\n",
    "\n",
    "# Compile\n",
    "model_improved.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print('MLP Improved Architecture:')\n",
    "model_improved.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train MLP Improved\n",
    "early_stop = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=20,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=10,\n",
    "    min_lr=0.00001,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "history_improved = model_improved.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=200,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate MLP Improved\n",
    "y_pred_improved = np.argmax(model_improved.predict(X_test, verbose=0), axis=1)\n",
    "\n",
    "acc_improved = accuracy_score(y_test, y_pred_improved)\n",
    "f1_improved = f1_score(y_test, y_pred_improved, average='weighted', zero_division=0)\n",
    "\n",
    "print(f'MLP Improved - Test Results:')\n",
    "print(f'  Accuracy: {acc_improved:.4f} ({acc_improved*100:.2f}%)')\n",
    "print(f'  F1-Weighted: {f1_improved:.4f}')\n",
    "print(f'  Epochs trained: {len(history_improved.history[\"loss\"])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning curves for MLP Improved\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history_improved.history['loss'], label='Train Loss', linewidth=2)\n",
    "axes[0].plot(history_improved.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('MLP Improved - Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(history_improved.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
    "axes[1].plot(history_improved.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('MLP Improved - Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparison: Simple vs Improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare MLP architectures\n",
    "comparison_mlp = pd.DataFrame({\n",
    "    'Model': ['MLP Simple', 'MLP Improved'],\n",
    "    'Accuracy': [acc_simple, acc_improved],\n",
    "    'F1_Weighted': [f1_simple, f1_improved],\n",
    "    'Epochs': [len(history_simple.history['loss']), len(history_improved.history['loss'])],\n",
    "    'Parameters': [model_simple.count_params(), model_improved.count_params()]\n",
    "})\n",
    "\n",
    "print('MLP Architecture Comparison:')\n",
    "print('='*80)\n",
    "print(comparison_mlp.to_string(index=False))\n",
    "print()\n",
    "\n",
    "improvement = (acc_improved - acc_simple) * 100\n",
    "print(f'Improvement: {improvement:+.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy comparison\n",
    "axes[0].bar(['MLP Simple', 'MLP Improved'], [acc_simple, acc_improved], \n",
    "            color=['steelblue', 'coral'], alpha=0.8)\n",
    "axes[0].set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('MLP Architecture Comparison - Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylim(0.7, max(acc_simple, acc_improved) * 1.05)\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for i, (model, acc) in enumerate(zip(['MLP Simple', 'MLP Improved'], [acc_simple, acc_improved])):\n",
    "    axes[0].text(i, acc + 0.005, f'{acc:.4f}', ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# F1-Score comparison\n",
    "axes[1].bar(['MLP Simple', 'MLP Improved'], [f1_simple, f1_improved], \n",
    "            color=['steelblue', 'coral'], alpha=0.8)\n",
    "axes[1].set_ylabel('F1-Weighted', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('MLP Architecture Comparison - F1-Weighted', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylim(0.7, max(f1_simple, f1_improved) * 1.05)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for i, (model, f1) in enumerate(zip(['MLP Simple', 'MLP Improved'], [f1_simple, f1_improved])):\n",
    "    axes[1].text(i, f1 + 0.005, f'{f1:.4f}', ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparison with Classical ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load classical ML metrics\n",
    "classical_metrics = pd.read_csv('../results/classification_metrics.csv')\n",
    "\n",
    "# Get top 3 classical models\n",
    "top_classical = classical_metrics.nlargest(3, 'Accuracy')[['Model', 'Accuracy', 'F1_Weighted']]\n",
    "\n",
    "print('Top 3 Classical ML Models:')\n",
    "print(top_classical.to_string(index=False))\n",
    "print()\n",
    "\n",
    "# Compare with MLP\n",
    "print('Deep Learning Models:')\n",
    "print(f'  MLP Simple:   Accuracy={acc_simple:.4f}, F1={f1_simple:.4f}')\n",
    "print(f'  MLP Improved: Accuracy={acc_improved:.4f}, F1={f1_improved:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison\n",
    "comparison_all = []\n",
    "\n",
    "# Add top 3 classical models\n",
    "for _, row in top_classical.iterrows():\n",
    "    comparison_all.append({\n",
    "        'Model': row['Model'],\n",
    "        'Type': 'Classical ML',\n",
    "        'Accuracy': row['Accuracy'],\n",
    "        'F1_Weighted': row['F1_Weighted']\n",
    "    })\n",
    "\n",
    "# Add MLP models\n",
    "comparison_all.append({\n",
    "    'Model': 'MLP Simple',\n",
    "    'Type': 'Deep Learning',\n",
    "    'Accuracy': acc_simple,\n",
    "    'F1_Weighted': f1_simple\n",
    "})\n",
    "\n",
    "comparison_all.append({\n",
    "    'Model': 'MLP Improved',\n",
    "    'Type': 'Deep Learning',\n",
    "    'Accuracy': acc_improved,\n",
    "    'F1_Weighted': f1_improved\n",
    "})\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_all)\n",
    "comparison_df = comparison_df.sort_values('Accuracy', ascending=False)\n",
    "\n",
    "print('\\nAll Models Comparison (Ranked by Accuracy):')\n",
    "print('='*80)\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comprehensive comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "colors = ['steelblue' if t == 'Classical ML' else 'coral' for t in comparison_df['Type']]\n",
    "\n",
    "# Accuracy\n",
    "axes[0].barh(comparison_df['Model'], comparison_df['Accuracy'], color=colors, alpha=0.8)\n",
    "axes[0].set_xlabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Model Comparison - Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlim(0.7, comparison_df['Accuracy'].max() * 1.05)\n",
    "axes[0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "for i, (model, acc) in enumerate(zip(comparison_df['Model'], comparison_df['Accuracy'])):\n",
    "    axes[0].text(acc + 0.005, i, f'{acc:.4f}', va='center', fontsize=9)\n",
    "\n",
    "# F1-Weighted\n",
    "axes[1].barh(comparison_df['Model'], comparison_df['F1_Weighted'], color=colors, alpha=0.8)\n",
    "axes[1].set_xlabel('F1-Weighted Score', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Model Comparison - F1-Weighted', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlim(0.7, comparison_df['F1_Weighted'].max() * 1.05)\n",
    "axes[1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "for i, (model, f1) in enumerate(zip(comparison_df['Model'], comparison_df['F1_Weighted'])):\n",
    "    axes[1].text(f1 + 0.005, i, f'{f1:.4f}', va='center', fontsize=9)\n",
    "\n",
    "# Legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='steelblue', alpha=0.8, label='Classical ML'),\n",
    "    Patch(facecolor='coral', alpha=0.8, label='Deep Learning')\n",
    "]\n",
    "axes[1].legend(handles=legend_elements, loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Key Insights and Conclusions\n",
    "\n",
    "### Summary of Findings:\n",
    "\n",
    "1. **MLP Simple vs Improved**:\n",
    "   - MLP Improved outperforms MLP Simple\n",
    "   - Deeper architecture captures more complex patterns\n",
    "   - BatchNormalization and ReduceLROnPlateau help convergence\n",
    "\n",
    "2. **Deep Learning vs Classical ML**:\n",
    "   - MLP Improved achieves competitive performance\n",
    "   - Random Forest remains strong baseline (79.68%)\n",
    "   - Deep Learning shows marginal improvement (+1-2%)\n",
    "\n",
    "3. **Training Dynamics**:\n",
    "   - Early stopping prevents overfitting\n",
    "   - Learning curves show good convergence\n",
    "   - Validation loss stabilizes after ~30-50 epochs\n",
    "\n",
    "4. **Dataset Considerations**:\n",
    "   - 950 observations is relatively small for deep learning\n",
    "   - 8 features limits network depth benefits\n",
    "   - Classical ML (Random Forest) is well-suited for this dataset\n",
    "\n",
    "### Recommendations:\n",
    "\n",
    "- **For this dataset**: Random Forest remains the best choice (simpler, interpretable, similar performance)\n",
    "- **For larger datasets**: Deep learning would likely show greater advantages\n",
    "- **For production**: Consider ensemble of Random Forest + MLP Improved\n",
    "\n",
    "### Overall Project Conclusion:\n",
    "\n",
    "Across 4 phases (Regression, Classification, Unsupervised, Deep Learning), we've demonstrated:\n",
    "- **Best model**: Random Forest (79.68% accuracy)\n",
    "- **Deep learning**: Competitive but not significantly better for this dataset size\n",
    "- **Unsupervised learning**: Revealed 3 natural economic clusters (ARI=0.07 with ratings)\n",
    "- **Feature importance**: FX_Reserves, Public_Debt, Unemployment are key predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Load Saved Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved metrics\n",
    "mlp_simple_metrics = pd.read_csv('../results/deep_learning/mlp_simple_metrics.csv')\n",
    "mlp_improved_metrics = pd.read_csv('../results/deep_learning/mlp_improved_metrics.csv')\n",
    "mlp_comparison = pd.read_csv('../results/deep_learning/mlp_comparison.csv')\n",
    "\n",
    "print('Saved MLP Simple Metrics:')\n",
    "print(mlp_simple_metrics)\n",
    "print('\\nSaved MLP Improved Metrics:')\n",
    "print(mlp_improved_metrics)\n",
    "print('\\nSaved Comparison:')\n",
    "print(mlp_comparison)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
