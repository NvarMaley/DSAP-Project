{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Classification - Credit Rating Prediction\n",
    "\n",
    "This notebook implements and evaluates classification models for predicting sovereign credit ratings as categorical labels (AAA, BB+, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, cross_val_predict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset with credit rating labels\n",
    "df = pd.read_csv('../data/processed/merged_dataset_labels.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features (X) and target (y)\n",
    "X = df.drop(['Country', 'Year', 'Credit_Rating_Label'], axis=1)\n",
    "y = df['Credit_Rating_Label']\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nNumber of classes: {y.nunique()}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "y.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. k-Nearest Neighbors (k-NN) Classification\n",
    "\n",
    "k-NN predicts the credit rating by finding the k most similar countries (based on macroeconomic indicators) and taking the majority vote.\n",
    "\n",
    "**Key points:**\n",
    "- Feature normalization is **mandatory** (k-NN uses Euclidean distance)\n",
    "- We test k = 3, 5, 7, 9\n",
    "- Stratified K-Fold CV (K=5) to maintain class distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 k-NN with k=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features\n",
    "scaler_k3 = StandardScaler()\n",
    "X_scaled_k3 = scaler_k3.fit_transform(X)\n",
    "\n",
    "# Create k-NN model with k=3\n",
    "knn_k3 = KNeighborsClassifier(n_neighbors=3, metric='euclidean')\n",
    "\n",
    "# Stratified K-Fold CV\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Cross-validation scores\n",
    "accuracy_k3 = cross_val_score(knn_k3, X_scaled_k3, y, cv=skf, scoring='accuracy')\n",
    "f1_weighted_k3 = cross_val_score(knn_k3, X_scaled_k3, y, cv=skf, scoring='f1_weighted')\n",
    "f1_macro_k3 = cross_val_score(knn_k3, X_scaled_k3, y, cv=skf, scoring='f1_macro')\n",
    "\n",
    "print(\"k-NN (k=3) Cross-Validation Results:\")\n",
    "print(f\"  Accuracy:      {accuracy_k3.mean():.4f} ± {accuracy_k3.std():.4f}\")\n",
    "print(f\"  F1-Weighted:   {f1_weighted_k3.mean():.4f} ± {f1_weighted_k3.std():.4f}\")\n",
    "print(f\"  F1-Macro:      {f1_macro_k3.mean():.4f} ± {f1_macro_k3.std():.4f}\")\n",
    "\n",
    "# Train final model\n",
    "knn_k3.fit(X_scaled_k3, y)\n",
    "print(\"\\n✓ Model trained on full dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix for k=3\n",
    "y_pred_k3 = cross_val_predict(knn_k3, X_scaled_k3, y, cv=skf)\n",
    "cm_k3 = confusion_matrix(y, y_pred_k3)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(14, 12))\n",
    "labels = sorted(y.unique())\n",
    "sns.heatmap(cm_k3, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=labels, yticklabels=labels,\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title(f'k-NN (k=3) Confusion Matrix\\nAccuracy: {accuracy_score(y, y_pred_k3):.4f}', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report for k=3\n",
    "print(\"Classification Report (k=3):\")\n",
    "print(classification_report(y, y_pred_k3, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 k-NN with k=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN with k=5\n",
    "scaler_k5 = StandardScaler()\n",
    "X_scaled_k5 = scaler_k5.fit_transform(X)\n",
    "\n",
    "knn_k5 = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
    "\n",
    "accuracy_k5 = cross_val_score(knn_k5, X_scaled_k5, y, cv=skf, scoring='accuracy')\n",
    "f1_weighted_k5 = cross_val_score(knn_k5, X_scaled_k5, y, cv=skf, scoring='f1_weighted')\n",
    "f1_macro_k5 = cross_val_score(knn_k5, X_scaled_k5, y, cv=skf, scoring='f1_macro')\n",
    "\n",
    "print(\"k-NN (k=5) Cross-Validation Results:\")\n",
    "print(f\"  Accuracy:      {accuracy_k5.mean():.4f} ± {accuracy_k5.std():.4f}\")\n",
    "print(f\"  F1-Weighted:   {f1_weighted_k5.mean():.4f} ± {f1_weighted_k5.std():.4f}\")\n",
    "print(f\"  F1-Macro:      {f1_macro_k5.mean():.4f} ± {f1_macro_k5.std():.4f}\")\n",
    "\n",
    "knn_k5.fit(X_scaled_k5, y)\n",
    "y_pred_k5 = cross_val_predict(knn_k5, X_scaled_k5, y, cv=skf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 k-NN with k=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN with k=7\n",
    "scaler_k7 = StandardScaler()\n",
    "X_scaled_k7 = scaler_k7.fit_transform(X)\n",
    "\n",
    "knn_k7 = KNeighborsClassifier(n_neighbors=7, metric='euclidean')\n",
    "\n",
    "accuracy_k7 = cross_val_score(knn_k7, X_scaled_k7, y, cv=skf, scoring='accuracy')\n",
    "f1_weighted_k7 = cross_val_score(knn_k7, X_scaled_k7, y, cv=skf, scoring='f1_weighted')\n",
    "f1_macro_k7 = cross_val_score(knn_k7, X_scaled_k7, y, cv=skf, scoring='f1_macro')\n",
    "\n",
    "print(\"k-NN (k=7) Cross-Validation Results:\")\n",
    "print(f\"  Accuracy:      {accuracy_k7.mean():.4f} ± {accuracy_k7.std():.4f}\")\n",
    "print(f\"  F1-Weighted:   {f1_weighted_k7.mean():.4f} ± {f1_weighted_k7.std():.4f}\")\n",
    "print(f\"  F1-Macro:      {f1_macro_k7.mean():.4f} ± {f1_macro_k7.std():.4f}\")\n",
    "\n",
    "knn_k7.fit(X_scaled_k7, y)\n",
    "y_pred_k7 = cross_val_predict(knn_k7, X_scaled_k7, y, cv=skf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 k-NN with k=9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN with k=9\n",
    "scaler_k9 = StandardScaler()\n",
    "X_scaled_k9 = scaler_k9.fit_transform(X)\n",
    "\n",
    "knn_k9 = KNeighborsClassifier(n_neighbors=9, metric='euclidean')\n",
    "\n",
    "accuracy_k9 = cross_val_score(knn_k9, X_scaled_k9, y, cv=skf, scoring='accuracy')\n",
    "f1_weighted_k9 = cross_val_score(knn_k9, X_scaled_k9, y, cv=skf, scoring='f1_weighted')\n",
    "f1_macro_k9 = cross_val_score(knn_k9, X_scaled_k9, y, cv=skf, scoring='f1_macro')\n",
    "\n",
    "print(\"k-NN (k=9) Cross-Validation Results:\")\n",
    "print(f\"  Accuracy:      {accuracy_k9.mean():.4f} ± {accuracy_k9.std():.4f}\")\n",
    "print(f\"  F1-Weighted:   {f1_weighted_k9.mean():.4f} ± {f1_weighted_k9.std():.4f}\")\n",
    "print(f\"  F1-Macro:      {f1_macro_k9.mean():.4f} ± {f1_macro_k9.std():.4f}\")\n",
    "\n",
    "knn_k9.fit(X_scaled_k9, y)\n",
    "y_pred_k9 = cross_val_predict(knn_k9, X_scaled_k9, y, cv=skf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comparison of k Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame({\n",
    "    'k': [3, 5, 7, 9],\n",
    "    'Accuracy': [accuracy_k3.mean(), accuracy_k5.mean(), accuracy_k7.mean(), accuracy_k9.mean()],\n",
    "    'Accuracy_Std': [accuracy_k3.std(), accuracy_k5.std(), accuracy_k7.std(), accuracy_k9.std()],\n",
    "    'F1_Weighted': [f1_weighted_k3.mean(), f1_weighted_k5.mean(), f1_weighted_k7.mean(), f1_weighted_k9.mean()],\n",
    "    'F1_Weighted_Std': [f1_weighted_k3.std(), f1_weighted_k5.std(), f1_weighted_k7.std(), f1_weighted_k9.std()],\n",
    "    'F1_Macro': [f1_macro_k3.mean(), f1_macro_k5.mean(), f1_macro_k7.mean(), f1_macro_k9.mean()],\n",
    "    'F1_Macro_Std': [f1_macro_k3.std(), f1_macro_k5.std(), f1_macro_k7.std(), f1_macro_k9.std()]\n",
    "})\n",
    "\n",
    "print(\"k-NN Performance Comparison:\")\n",
    "print(\"=\"*80)\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Accuracy\n",
    "axes[0].errorbar(comparison_df['k'], comparison_df['Accuracy'], \n",
    "                 yerr=comparison_df['Accuracy_Std'], \n",
    "                 marker='o', capsize=5, capthick=2, linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel('k (Number of Neighbors)', fontsize=12)\n",
    "axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[0].set_title('Accuracy vs k', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_xticks([3, 5, 7, 9])\n",
    "\n",
    "# F1-Weighted\n",
    "axes[1].errorbar(comparison_df['k'], comparison_df['F1_Weighted'], \n",
    "                 yerr=comparison_df['F1_Weighted_Std'], \n",
    "                 marker='o', capsize=5, capthick=2, linewidth=2, markersize=8, color='orange')\n",
    "axes[1].set_xlabel('k (Number of Neighbors)', fontsize=12)\n",
    "axes[1].set_ylabel('F1-Weighted', fontsize=12)\n",
    "axes[1].set_title('F1-Weighted vs k', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_xticks([3, 5, 7, 9])\n",
    "\n",
    "# F1-Macro\n",
    "axes[2].errorbar(comparison_df['k'], comparison_df['F1_Macro'], \n",
    "                 yerr=comparison_df['F1_Macro_Std'], \n",
    "                 marker='o', capsize=5, capthick=2, linewidth=2, markersize=8, color='green')\n",
    "axes[2].set_xlabel('k (Number of Neighbors)', fontsize=12)\n",
    "axes[2].set_ylabel('F1-Macro', fontsize=12)\n",
    "axes[2].set_title('F1-Macro vs k', fontsize=14, fontweight='bold')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "axes[2].set_xticks([3, 5, 7, 9])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. k-NN Analysis and Conclusions\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Best k value**: k=3 achieves the highest performance\n",
    "   - Accuracy: ~68.84%\n",
    "   - F1-Weighted: ~68.12%\n",
    "   - F1-Macro: ~54.37%\n",
    "\n",
    "2. **Performance decreases as k increases**:\n",
    "   - k=3: Most precise, uses closest neighbors\n",
    "   - k=9: Too smooth, loses detail\n",
    "\n",
    "3. **Well-predicted classes** (k=3):\n",
    "   - AAA: 89% F1-Score (226 examples)\n",
    "   - AA+: 80% F1-Score (50 examples)\n",
    "   - BB: 78% F1-Score (12 examples)\n",
    "\n",
    "4. **Difficult classes** (k=3):\n",
    "   - CC, CCC, CCC+: 0% (only 3-4 examples each)\n",
    "   - Classes with <10 examples are very hard to predict\n",
    "\n",
    "5. **Overall performance**:\n",
    "   - 68.84% accuracy with 20 classes is excellent\n",
    "   - Random guessing would give ~5% accuracy\n",
    "   - k-NN is 13× better than random!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved metrics\n",
    "metrics_df = pd.read_csv('../results/classification_metrics.csv')\n",
    "print(\"Saved Classification Metrics:\")\n",
    "print(\"=\"*80)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Naive Bayes Classification\n",
    "\n",
    "Naive Bayes is a probabilistic classifier based on Bayes' theorem. It calculates the probability of each class given the features and predicts the class with the highest probability.\n",
    "\n",
    "**Key points:**\n",
    "- No feature normalization needed (unlike k-NN)\n",
    "- Assumes features are independent (naive assumption)\n",
    "- Fast training and prediction\n",
    "- Good probabilistic baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Gaussian Naive Bayes model\n",
    "nb_model = GaussianNB()\n",
    "\n",
    "# Cross-validation scores (no normalization needed)\n",
    "accuracy_nb = cross_val_score(nb_model, X, y, cv=skf, scoring='accuracy')\n",
    "f1_weighted_nb = cross_val_score(nb_model, X, y, cv=skf, scoring='f1_weighted')\n",
    "f1_macro_nb = cross_val_score(nb_model, X, y, cv=skf, scoring='f1_macro')\n",
    "\n",
    "print(\"Naive Bayes Cross-Validation Results:\")\n",
    "print(f\"  Accuracy:      {accuracy_nb.mean():.4f} ± {accuracy_nb.std():.4f}\")\n",
    "print(f\"  F1-Weighted:   {f1_weighted_nb.mean():.4f} ± {f1_weighted_nb.std():.4f}\")\n",
    "print(f\"  F1-Macro:      {f1_macro_nb.mean():.4f} ± {f1_macro_nb.std():.4f}\")\n",
    "\n",
    "# Train final model\n",
    "nb_model.fit(X, y)\n",
    "print(\"\\n✓ Model trained on full dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix for Naive Bayes\n",
    "y_pred_nb = cross_val_predict(nb_model, X, y, cv=skf)\n",
    "cm_nb = confusion_matrix(y, y_pred_nb)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(14, 12))\n",
    "labels = sorted(y.unique())\n",
    "sns.heatmap(cm_nb, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=labels, yticklabels=labels,\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title(f'Naive Bayes Confusion Matrix\\nAccuracy: {accuracy_score(y, y_pred_nb):.4f}', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report for Naive Bayes\n",
    "print(\"Classification Report (Naive Bayes):\")\n",
    "print(classification_report(y, y_pred_nb, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparison: k-NN vs Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe including Naive Bayes\n",
    "comparison_all_df = pd.DataFrame({\n",
    "    'Model': ['k-NN (k=3)', 'k-NN (k=5)', 'k-NN (k=7)', 'k-NN (k=9)', 'Naive Bayes'],\n",
    "    'Accuracy': [accuracy_k3.mean(), accuracy_k5.mean(), accuracy_k7.mean(), accuracy_k9.mean(), accuracy_nb.mean()],\n",
    "    'Accuracy_Std': [accuracy_k3.std(), accuracy_k5.std(), accuracy_k7.std(), accuracy_k9.std(), accuracy_nb.std()],\n",
    "    'F1_Weighted': [f1_weighted_k3.mean(), f1_weighted_k5.mean(), f1_weighted_k7.mean(), f1_weighted_k9.mean(), f1_weighted_nb.mean()],\n",
    "    'F1_Weighted_Std': [f1_weighted_k3.std(), f1_weighted_k5.std(), f1_weighted_k7.std(), f1_weighted_k9.std(), f1_weighted_nb.std()],\n",
    "    'F1_Macro': [f1_macro_k3.mean(), f1_macro_k5.mean(), f1_macro_k7.mean(), f1_macro_k9.mean(), f1_macro_nb.mean()],\n",
    "    'F1_Macro_Std': [f1_macro_k3.std(), f1_macro_k5.std(), f1_macro_k7.std(), f1_macro_k9.std(), f1_macro_nb.std()]\n",
    "})\n",
    "\n",
    "print(\"All Models Performance Comparison:\")\n",
    "print(\"=\"*80)\n",
    "comparison_all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "models = ['k-NN\\n(k=3)', 'k-NN\\n(k=5)', 'k-NN\\n(k=7)', 'k-NN\\n(k=9)', 'Naive\\nBayes']\n",
    "x_pos = np.arange(len(models))\n",
    "\n",
    "# Accuracy\n",
    "axes[0].bar(x_pos, comparison_all_df['Accuracy'], yerr=comparison_all_df['Accuracy_Std'], \n",
    "            capsize=5, alpha=0.7, color=['#1f77b4', '#1f77b4', '#1f77b4', '#1f77b4', '#ff7f0e'])\n",
    "axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[0].set_title('Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xticks(x_pos)\n",
    "axes[0].set_xticklabels(models)\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# F1-Weighted\n",
    "axes[1].bar(x_pos, comparison_all_df['F1_Weighted'], yerr=comparison_all_df['F1_Weighted_Std'], \n",
    "            capsize=5, alpha=0.7, color=['#1f77b4', '#1f77b4', '#1f77b4', '#1f77b4', '#ff7f0e'])\n",
    "axes[1].set_ylabel('F1-Weighted', fontsize=12)\n",
    "axes[1].set_title('F1-Weighted Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xticks(x_pos)\n",
    "axes[1].set_xticklabels(models)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# F1-Macro\n",
    "axes[2].bar(x_pos, comparison_all_df['F1_Macro'], yerr=comparison_all_df['F1_Macro_Std'], \n",
    "            capsize=5, alpha=0.7, color=['#1f77b4', '#1f77b4', '#1f77b4', '#1f77b4', '#ff7f0e'])\n",
    "axes[2].set_ylabel('F1-Macro', fontsize=12)\n",
    "axes[2].set_title('F1-Macro Comparison', fontsize=14, fontweight='bold')\n",
    "axes[2].set_xticks(x_pos)\n",
    "axes[2].set_xticklabels(models)\n",
    "axes[2].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Overall Analysis\n",
    "\n",
    "### Naive Bayes Performance:\n",
    "\n",
    "**Results:**\n",
    "- Accuracy: ~37.26%\n",
    "- F1-Weighted: ~37.32%\n",
    "- F1-Macro: ~30.67%\n",
    "\n",
    "**Why is Naive Bayes much weaker than k-NN?**\n",
    "\n",
    "1. **Independence assumption violated**: Naive Bayes assumes features are independent, but in reality:\n",
    "   - Interest_Rate and Inflation are correlated\n",
    "   - Unemployment and GDP_Growth are correlated\n",
    "   - This violates the \"naive\" assumption\n",
    "\n",
    "2. **20 classes with imbalanced data**: \n",
    "   - Classes with few examples (CC=3, CCC=4) are very difficult\n",
    "   - Gaussian assumption may not hold for all features\n",
    "\n",
    "3. **k-NN handles correlation naturally**: \n",
    "   - k-NN uses distances, not probabilities\n",
    "   - No independence assumption needed\n",
    "\n",
    "### Best Model So Far:\n",
    "\n",
    "**k-NN (k=3)** is the clear winner:\n",
    "- 68.84% accuracy (vs 37.26% for Naive Bayes)\n",
    "- 85% better than Naive Bayes\n",
    "- 13× better than random guessing (5%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved metrics\n",
    "metrics_df = pd.read_csv('../results/classification_metrics.csv')\n",
    "print(\"Saved Classification Metrics:\")\n",
    "print(\"=\"*80)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "In the following sections, we will implement and compare:\n",
    "- Decision Tree (interpretable model with max_depth tuning)\n",
    "- Random Forest (ensemble method for improved performance)\n",
    "\n",
    "These tree-based models should handle feature correlation better than Naive Bayes and potentially match or exceed k-NN performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Decision Tree Classification\n",
    "\n",
    "Decision Tree creates a tree-like model of decisions based on feature values. Each internal node represents a test on a feature, each branch represents the outcome, and each leaf node represents a class label.\n",
    "\n",
    "**Key points:**\n",
    "- No feature normalization needed\n",
    "- Highly interpretable (can visualize the tree)\n",
    "- Provides feature importance\n",
    "- Test max_depth = 3, 5, 10\n",
    "- Uses `class_weight='balanced'` to handle class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Decision Tree with max_depth=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Decision Tree model with max_depth=3\n",
    "dt_model_3 = DecisionTreeClassifier(max_depth=3, random_state=42, class_weight='balanced')\n",
    "\n",
    "# Cross-validation scores\n",
    "accuracy_dt3 = cross_val_score(dt_model_3, X, y, cv=skf, scoring='accuracy')\n",
    "f1_weighted_dt3 = cross_val_score(dt_model_3, X, y, cv=skf, scoring='f1_weighted')\n",
    "f1_macro_dt3 = cross_val_score(dt_model_3, X, y, cv=skf, scoring='f1_macro')\n",
    "\n",
    "print(\"Decision Tree (depth=3) Cross-Validation Results:\")\n",
    "print(f\"  Accuracy:      {accuracy_dt3.mean():.4f} ± {accuracy_dt3.std():.4f}\")\n",
    "print(f\"  F1-Weighted:   {f1_weighted_dt3.mean():.4f} ± {f1_weighted_dt3.std():.4f}\")\n",
    "print(f\"  F1-Macro:      {f1_macro_dt3.mean():.4f} ± {f1_macro_dt3.std():.4f}\")\n",
    "\n",
    "# Train final model\n",
    "dt_model_3.fit(X, y)\n",
    "print(\"\\n✓ Model trained on full dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance for depth=3\n",
    "feature_importance_dt3 = dt_model_3.feature_importances_\n",
    "importance_df_dt3 = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': feature_importance_dt3\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importance (depth=3):\")\n",
    "print(importance_df_dt3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Decision Tree with max_depth=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree with max_depth=5\n",
    "dt_model_5 = DecisionTreeClassifier(max_depth=5, random_state=42, class_weight='balanced')\n",
    "\n",
    "accuracy_dt5 = cross_val_score(dt_model_5, X, y, cv=skf, scoring='accuracy')\n",
    "f1_weighted_dt5 = cross_val_score(dt_model_5, X, y, cv=skf, scoring='f1_weighted')\n",
    "f1_macro_dt5 = cross_val_score(dt_model_5, X, y, cv=skf, scoring='f1_macro')\n",
    "\n",
    "print(\"Decision Tree (depth=5) Cross-Validation Results:\")\n",
    "print(f\"  Accuracy:      {accuracy_dt5.mean():.4f} ± {accuracy_dt5.std():.4f}\")\n",
    "print(f\"  F1-Weighted:   {f1_weighted_dt5.mean():.4f} ± {f1_weighted_dt5.std():.4f}\")\n",
    "print(f\"  F1-Macro:      {f1_macro_dt5.mean():.4f} ± {f1_macro_dt5.std():.4f}\")\n",
    "\n",
    "dt_model_5.fit(X, y)\n",
    "y_pred_dt5 = cross_val_predict(dt_model_5, X, y, cv=skf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Decision Tree with max_depth=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree with max_depth=10\n",
    "dt_model_10 = DecisionTreeClassifier(max_depth=10, random_state=42, class_weight='balanced')\n",
    "\n",
    "accuracy_dt10 = cross_val_score(dt_model_10, X, y, cv=skf, scoring='accuracy')\n",
    "f1_weighted_dt10 = cross_val_score(dt_model_10, X, y, cv=skf, scoring='f1_weighted')\n",
    "f1_macro_dt10 = cross_val_score(dt_model_10, X, y, cv=skf, scoring='f1_macro')\n",
    "\n",
    "print(\"Decision Tree (depth=10) Cross-Validation Results:\")\n",
    "print(f\"  Accuracy:      {accuracy_dt10.mean():.4f} ± {accuracy_dt10.std():.4f}\")\n",
    "print(f\"  F1-Weighted:   {f1_weighted_dt10.mean():.4f} ± {f1_weighted_dt10.std():.4f}\")\n",
    "print(f\"  F1-Macro:      {f1_macro_dt10.mean():.4f} ± {f1_macro_dt10.std():.4f}\")\n",
    "\n",
    "dt_model_10.fit(X, y)\n",
    "y_pred_dt10 = cross_val_predict(dt_model_10, X, y, cv=skf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance for depth=10 (Best Decision Tree)\n",
    "feature_importance_dt10 = dt_model_10.feature_importances_\n",
    "importance_df_dt10 = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': feature_importance_dt10\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df_dt10.sort_values('Importance')['Feature'], \n",
    "         importance_df_dt10.sort_values('Importance')['Importance'], \n",
    "         color='steelblue')\n",
    "plt.xlabel('Importance', fontsize=12)\n",
    "plt.ylabel('Feature', fontsize=12)\n",
    "plt.title('Decision Tree (depth=10) - Feature Importance', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFeature Importance Ranking:\")\n",
    "print(importance_df_dt10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Comparison of Decision Tree max_depth Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe for all models including Decision Trees\n",
    "comparison_all_models = pd.DataFrame({\n",
    "    'Model': ['k-NN (k=3)', 'k-NN (k=5)', 'k-NN (k=7)', 'k-NN (k=9)', \n",
    "              'Naive Bayes', 'DT (depth=3)', 'DT (depth=5)', 'DT (depth=10)'],\n",
    "    'Accuracy': [accuracy_k3.mean(), accuracy_k5.mean(), accuracy_k7.mean(), accuracy_k9.mean(), \n",
    "                 accuracy_nb.mean(), accuracy_dt3.mean(), accuracy_dt5.mean(), accuracy_dt10.mean()],\n",
    "    'Accuracy_Std': [accuracy_k3.std(), accuracy_k5.std(), accuracy_k7.std(), accuracy_k9.std(), \n",
    "                     accuracy_nb.std(), accuracy_dt3.std(), accuracy_dt5.std(), accuracy_dt10.std()],\n",
    "    'F1_Weighted': [f1_weighted_k3.mean(), f1_weighted_k5.mean(), f1_weighted_k7.mean(), f1_weighted_k9.mean(), \n",
    "                    f1_weighted_nb.mean(), f1_weighted_dt3.mean(), f1_weighted_dt5.mean(), f1_weighted_dt10.mean()],\n",
    "    'F1_Weighted_Std': [f1_weighted_k3.std(), f1_weighted_k5.std(), f1_weighted_k7.std(), f1_weighted_k9.std(), \n",
    "                        f1_weighted_nb.std(), f1_weighted_dt3.std(), f1_weighted_dt5.std(), f1_weighted_dt10.std()]\n",
    "})\n",
    "\n",
    "print(\"All Models Performance Comparison:\")\n",
    "print(\"=\"*80)\n",
    "comparison_all_models.sort_values('Accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison of all models\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "models = comparison_all_models['Model']\n",
    "x_pos = np.arange(len(models))\n",
    "\n",
    "# Accuracy\n",
    "axes[0].bar(x_pos, comparison_all_models['Accuracy'], \n",
    "            yerr=comparison_all_models['Accuracy_Std'], \n",
    "            capsize=5, alpha=0.7, \n",
    "            color=['#1f77b4', '#1f77b4', '#1f77b4', '#1f77b4', '#ff7f0e', '#2ca02c', '#2ca02c', '#2ca02c'])\n",
    "axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[0].set_title('Accuracy Comparison - All Models', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xticks(x_pos)\n",
    "axes[0].set_xticklabels(models, rotation=45, ha='right')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# F1-Weighted\n",
    "axes[1].bar(x_pos, comparison_all_models['F1_Weighted'], \n",
    "            yerr=comparison_all_models['F1_Weighted_Std'], \n",
    "            capsize=5, alpha=0.7, \n",
    "            color=['#1f77b4', '#1f77b4', '#1f77b4', '#1f77b4', '#ff7f0e', '#2ca02c', '#2ca02c', '#2ca02c'])\n",
    "axes[1].set_ylabel('F1-Weighted', fontsize=12)\n",
    "axes[1].set_title('F1-Weighted Comparison - All Models', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xticks(x_pos)\n",
    "axes[1].set_xticklabels(models, rotation=45, ha='right')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Decision Tree Analysis\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Performance varies significantly with max_depth**:\n",
    "   - depth=3: Very poor (8% accuracy) - too simple, over-compensates for class imbalance\n",
    "   - depth=5: Poor (23% accuracy) - still too simple\n",
    "   - depth=10: Good (52% accuracy) - best Decision Tree performance\n",
    "\n",
    "2. **Decision Tree (depth=10) vs other models**:\n",
    "   - Better than Naive Bayes (37%)\n",
    "   - Worse than all k-NN variants (60-69%)\n",
    "   - Best single Decision Tree: depth=10\n",
    "\n",
    "3. **Feature Importance (depth=10)**:\n",
    "   - **FX_Reserves**: 23.63% (most important!)\n",
    "   - **Public_Debt**: 21.28%\n",
    "   - **Interest_Rate**: 14.54%\n",
    "   - **Unemployment**: 14.51%\n",
    "   \n",
    "4. **Insights**:\n",
    "   - Foreign exchange reserves and public debt are the strongest predictors\n",
    "   - Decision Trees handle feature correlation naturally\n",
    "   - Deeper trees perform better but risk overfitting\n",
    "\n",
    "### Current Best Model: k-NN (k=3) with 68.84% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Random Forest Classification\n",
    "\n",
    "Random Forest is an ensemble learning method that combines multiple decision trees. Each tree is trained on a random subset of the data and features, and the final prediction is made by majority voting.\n",
    "\n",
    "**Key points:**\n",
    "- Ensemble of decision trees (Bagging)\n",
    "- No feature normalization needed\n",
    "- Reduces overfitting compared to single Decision Tree\n",
    "- Test n_estimators = 50, 100, 200\n",
    "- Provides feature importance\n",
    "- Uses `class_weight='balanced'` and `n_jobs=-1` (parallel processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Random Forest with n_estimators=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Random Forest model with n_estimators=50\n",
    "rf_model_50 = RandomForestClassifier(n_estimators=50, max_depth=None, random_state=42, \n",
    "                                      class_weight='balanced', n_jobs=-1)\n",
    "\n",
    "# Cross-validation scores\n",
    "accuracy_rf50 = cross_val_score(rf_model_50, X, y, cv=skf, scoring='accuracy')\n",
    "f1_weighted_rf50 = cross_val_score(rf_model_50, X, y, cv=skf, scoring='f1_weighted')\n",
    "f1_macro_rf50 = cross_val_score(rf_model_50, X, y, cv=skf, scoring='f1_macro')\n",
    "\n",
    "print(\"Random Forest (n=50) Cross-Validation Results:\")\n",
    "print(f\"  Accuracy:      {accuracy_rf50.mean():.4f} ± {accuracy_rf50.std():.4f}\")\n",
    "print(f\"  F1-Weighted:   {f1_weighted_rf50.mean():.4f} ± {f1_weighted_rf50.std():.4f}\")\n",
    "print(f\"  F1-Macro:      {f1_macro_rf50.mean():.4f} ± {f1_macro_rf50.std():.4f}\")\n",
    "\n",
    "# Train final model\n",
    "rf_model_50.fit(X, y)\n",
    "print(\"\\n✓ Model trained on full dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Random Forest with n_estimators=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest with n_estimators=100\n",
    "rf_model_100 = RandomForestClassifier(n_estimators=100, max_depth=None, random_state=42, \n",
    "                                       class_weight='balanced', n_jobs=-1)\n",
    "\n",
    "accuracy_rf100 = cross_val_score(rf_model_100, X, y, cv=skf, scoring='accuracy')\n",
    "f1_weighted_rf100 = cross_val_score(rf_model_100, X, y, cv=skf, scoring='f1_weighted')\n",
    "f1_macro_rf100 = cross_val_score(rf_model_100, X, y, cv=skf, scoring='f1_macro')\n",
    "\n",
    "print(\"Random Forest (n=100) Cross-Validation Results:\")\n",
    "print(f\"  Accuracy:      {accuracy_rf100.mean():.4f} ± {accuracy_rf100.std():.4f}\")\n",
    "print(f\"  F1-Weighted:   {f1_weighted_rf100.mean():.4f} ± {f1_weighted_rf100.std():.4f}\")\n",
    "print(f\"  F1-Macro:      {f1_macro_rf100.mean():.4f} ± {f1_macro_rf100.std():.4f}\")\n",
    "\n",
    "rf_model_100.fit(X, y)\n",
    "y_pred_rf100 = cross_val_predict(rf_model_100, X, y, cv=skf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 Random Forest with n_estimators=200 (Best Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest with n_estimators=200\n",
    "rf_model_200 = RandomForestClassifier(n_estimators=200, max_depth=None, random_state=42, \n",
    "                                       class_weight='balanced', n_jobs=-1)\n",
    "\n",
    "accuracy_rf200 = cross_val_score(rf_model_200, X, y, cv=skf, scoring='accuracy')\n",
    "f1_weighted_rf200 = cross_val_score(rf_model_200, X, y, cv=skf, scoring='f1_weighted')\n",
    "f1_macro_rf200 = cross_val_score(rf_model_200, X, y, cv=skf, scoring='f1_macro')\n",
    "\n",
    "print(\"Random Forest (n=200) Cross-Validation Results:\")\n",
    "print(f\"  Accuracy:      {accuracy_rf200.mean():.4f} ± {accuracy_rf200.std():.4f}\")\n",
    "print(f\"  F1-Weighted:   {f1_weighted_rf200.mean():.4f} ± {f1_weighted_rf200.std():.4f}\")\n",
    "print(f\"  F1-Macro:      {f1_macro_rf200.mean():.4f} ± {f1_macro_rf200.std():.4f}\")\n",
    "\n",
    "rf_model_200.fit(X, y)\n",
    "y_pred_rf200 = cross_val_predict(rf_model_200, X, y, cv=skf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance for Random Forest (n=200)\n",
    "feature_importance_rf200 = rf_model_200.feature_importances_\n",
    "importance_df_rf200 = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': feature_importance_rf200\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df_rf200.sort_values('Importance')['Feature'], \n",
    "         importance_df_rf200.sort_values('Importance')['Importance'], \n",
    "         color='forestgreen')\n",
    "plt.xlabel('Importance', fontsize=12)\n",
    "plt.ylabel('Feature', fontsize=12)\n",
    "plt.title('Random Forest (n=200) - Feature Importance', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFeature Importance Ranking:\")\n",
    "print(importance_df_rf200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4 Comparison of Random Forest n_estimators Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final comparison dataframe for ALL models\n",
    "comparison_final = pd.DataFrame({\n",
    "    'Model': ['k-NN (k=3)', 'k-NN (k=5)', 'k-NN (k=7)', 'k-NN (k=9)', \n",
    "              'Naive Bayes', 'DT (depth=3)', 'DT (depth=5)', 'DT (depth=10)',\n",
    "              'RF (n=50)', 'RF (n=100)', 'RF (n=200)'],\n",
    "    'Accuracy': [accuracy_k3.mean(), accuracy_k5.mean(), accuracy_k7.mean(), accuracy_k9.mean(), \n",
    "                 accuracy_nb.mean(), accuracy_dt3.mean(), accuracy_dt5.mean(), accuracy_dt10.mean(),\n",
    "                 accuracy_rf50.mean(), accuracy_rf100.mean(), accuracy_rf200.mean()],\n",
    "    'Accuracy_Std': [accuracy_k3.std(), accuracy_k5.std(), accuracy_k7.std(), accuracy_k9.std(), \n",
    "                     accuracy_nb.std(), accuracy_dt3.std(), accuracy_dt5.std(), accuracy_dt10.std(),\n",
    "                     accuracy_rf50.std(), accuracy_rf100.std(), accuracy_rf200.std()],\n",
    "    'F1_Weighted': [f1_weighted_k3.mean(), f1_weighted_k5.mean(), f1_weighted_k7.mean(), f1_weighted_k9.mean(), \n",
    "                    f1_weighted_nb.mean(), f1_weighted_dt3.mean(), f1_weighted_dt5.mean(), f1_weighted_dt10.mean(),\n",
    "                    f1_weighted_rf50.mean(), f1_weighted_rf100.mean(), f1_weighted_rf200.mean()],\n",
    "    'F1_Weighted_Std': [f1_weighted_k3.std(), f1_weighted_k5.std(), f1_weighted_k7.std(), f1_weighted_k9.std(), \n",
    "                        f1_weighted_nb.std(), f1_weighted_dt3.std(), f1_weighted_dt5.std(), f1_weighted_dt10.std(),\n",
    "                        f1_weighted_rf50.std(), f1_weighted_rf100.std(), f1_weighted_rf200.std()]\n",
    "})\n",
    "\n",
    "print(\"FINAL Performance Comparison - All Models:\")\n",
    "print(\"=\"*80)\n",
    "comparison_final.sort_values('Accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize final comparison of all models\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "models = comparison_final['Model']\n",
    "x_pos = np.arange(len(models))\n",
    "\n",
    "# Color coding: k-NN (blue), Naive Bayes (orange), Decision Tree (green), Random Forest (red)\n",
    "colors = ['#1f77b4', '#1f77b4', '#1f77b4', '#1f77b4', '#ff7f0e', \n",
    "          '#2ca02c', '#2ca02c', '#2ca02c', '#d62728', '#d62728', '#d62728']\n",
    "\n",
    "# Accuracy\n",
    "axes[0].bar(x_pos, comparison_final['Accuracy'], \n",
    "            yerr=comparison_final['Accuracy_Std'], \n",
    "            capsize=5, alpha=0.7, color=colors)\n",
    "axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[0].set_title('Accuracy Comparison - All Models', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xticks(x_pos)\n",
    "axes[0].set_xticklabels(models, rotation=45, ha='right')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "axes[0].axhline(y=0.8, color='red', linestyle='--', alpha=0.5, label='80% threshold')\n",
    "axes[0].legend()\n",
    "\n",
    "# F1-Weighted\n",
    "axes[1].bar(x_pos, comparison_final['F1_Weighted'], \n",
    "            yerr=comparison_final['F1_Weighted_Std'], \n",
    "            capsize=5, alpha=0.7, color=colors)\n",
    "axes[1].set_ylabel('F1-Weighted', fontsize=12)\n",
    "axes[1].set_title('F1-Weighted Comparison - All Models', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xticks(x_pos)\n",
    "axes[1].set_xticklabels(models, rotation=45, ha='right')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "axes[1].axhline(y=0.8, color='red', linestyle='--', alpha=0.5, label='80% threshold')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Final Analysis and Conclusions\n",
    "\n",
    "### Random Forest Performance:\n",
    "\n",
    "**Results:**\n",
    "- **n=50**: 78.42% accuracy, 77.20% F1-weighted\n",
    "- **n=100**: 78.95% accuracy, 77.52% F1-weighted\n",
    "- **n=200**: **79.68% accuracy, 78.47% F1-weighted** ← **BEST MODEL**\n",
    "\n",
    "**Why Random Forest is the best:**\n",
    "\n",
    "1. **Ensemble Learning**: Combines 200 decision trees through majority voting\n",
    "   - Each tree sees different data (bootstrap sampling)\n",
    "   - Each tree uses random feature subsets\n",
    "   - Reduces overfitting dramatically\n",
    "\n",
    "2. **Performance comparison**:\n",
    "   - **10.84 points better** than k-NN (k=3): 79.68% vs 68.84%\n",
    "   - **27.89 points better** than Decision Tree (depth=10): 79.68% vs 51.79%\n",
    "   - **42.42 points better** than Naive Bayes: 79.68% vs 37.26%\n",
    "\n",
    "3. **Feature Importance (n=200)**:\n",
    "   - **FX_Reserves**: 16.97% (most important)\n",
    "   - **Public_Debt**: 15.45%\n",
    "   - **Unemployment**: 13.86%\n",
    "   - **Interest_Rate**: 13.53%\n",
    "   - All features contribute (balanced usage)\n",
    "\n",
    "4. **Stability**:\n",
    "   - Low standard deviation: ±2.6%\n",
    "   - Consistent across all 5 folds\n",
    "   - More stable than k-NN (±2.8%)\n",
    "\n",
    "### Model Type Comparison:\n",
    "\n",
    "| Model Type | Best Model | Accuracy | Strength |\n",
    "|------------|------------|----------|----------|\n",
    "| **Ensemble (Bagging)** | **Random Forest (n=200)** | **79.68%** | **Winner** |\n",
    "| Distance-based | k-NN (k=3) | 68.84% | Good |\n",
    "| Tree-based | Decision Tree (depth=10) | 51.79% | Interpretable |\n",
    "| Probabilistic | Naive Bayes | 37.26% | Fast baseline |\n",
    "\n",
    "### Key Insights:\n",
    "\n",
    "1. **Ensemble methods dominate**: Random Forest significantly outperforms all single models\n",
    "2. **Feature importance**: Foreign exchange reserves and public debt are the strongest predictors\n",
    "3. **Class imbalance handled well**: `class_weight='balanced'` helps with rare classes\n",
    "4. **Cross-validation essential**: Provides reliable performance estimates\n",
    "\n",
    "### Final Recommendation:\n",
    "\n",
    "**Use Random Forest (n=200) for production** with 79.68% accuracy and 78.47% F1-weighted score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display saved metrics\n",
    "metrics_df = pd.read_csv('../results/classification_metrics.csv')\n",
    "print(\"Saved Classification Metrics:\")\n",
    "print(\"=\"*80)\n",
    "metrics_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
