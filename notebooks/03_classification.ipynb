{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Classification - Credit Rating Prediction\n",
    "\n",
    "This notebook implements and evaluates classification models for predicting sovereign credit ratings as categorical labels (AAA, BB+, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, cross_val_predict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset with credit rating labels\n",
    "df = pd.read_csv('../data/processed/merged_dataset_labels.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features (X) and target (y)\n",
    "X = df.drop(['Country', 'Year', 'Credit_Rating_Label'], axis=1)\n",
    "y = df['Credit_Rating_Label']\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nNumber of classes: {y.nunique()}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "y.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. k-Nearest Neighbors (k-NN) Classification\n",
    "\n",
    "k-NN predicts the credit rating by finding the k most similar countries (based on macroeconomic indicators) and taking the majority vote.\n",
    "\n",
    "**Key points:**\n",
    "- Feature normalization is **mandatory** (k-NN uses Euclidean distance)\n",
    "- We test k = 3, 5, 7, 9\n",
    "- Stratified K-Fold CV (K=5) to maintain class distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 k-NN with k=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features\n",
    "scaler_k3 = StandardScaler()\n",
    "X_scaled_k3 = scaler_k3.fit_transform(X)\n",
    "\n",
    "# Create k-NN model with k=3\n",
    "knn_k3 = KNeighborsClassifier(n_neighbors=3, metric='euclidean')\n",
    "\n",
    "# Stratified K-Fold CV\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Cross-validation scores\n",
    "accuracy_k3 = cross_val_score(knn_k3, X_scaled_k3, y, cv=skf, scoring='accuracy')\n",
    "f1_weighted_k3 = cross_val_score(knn_k3, X_scaled_k3, y, cv=skf, scoring='f1_weighted')\n",
    "f1_macro_k3 = cross_val_score(knn_k3, X_scaled_k3, y, cv=skf, scoring='f1_macro')\n",
    "\n",
    "print(\"k-NN (k=3) Cross-Validation Results:\")\n",
    "print(f\"  Accuracy:      {accuracy_k3.mean():.4f} ± {accuracy_k3.std():.4f}\")\n",
    "print(f\"  F1-Weighted:   {f1_weighted_k3.mean():.4f} ± {f1_weighted_k3.std():.4f}\")\n",
    "print(f\"  F1-Macro:      {f1_macro_k3.mean():.4f} ± {f1_macro_k3.std():.4f}\")\n",
    "\n",
    "# Train final model\n",
    "knn_k3.fit(X_scaled_k3, y)\n",
    "print(\"\\n✓ Model trained on full dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix for k=3\n",
    "y_pred_k3 = cross_val_predict(knn_k3, X_scaled_k3, y, cv=skf)\n",
    "cm_k3 = confusion_matrix(y, y_pred_k3)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(14, 12))\n",
    "labels = sorted(y.unique())\n",
    "sns.heatmap(cm_k3, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=labels, yticklabels=labels,\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title(f'k-NN (k=3) Confusion Matrix\\nAccuracy: {accuracy_score(y, y_pred_k3):.4f}', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report for k=3\n",
    "print(\"Classification Report (k=3):\")\n",
    "print(classification_report(y, y_pred_k3, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 k-NN with k=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN with k=5\n",
    "scaler_k5 = StandardScaler()\n",
    "X_scaled_k5 = scaler_k5.fit_transform(X)\n",
    "\n",
    "knn_k5 = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
    "\n",
    "accuracy_k5 = cross_val_score(knn_k5, X_scaled_k5, y, cv=skf, scoring='accuracy')\n",
    "f1_weighted_k5 = cross_val_score(knn_k5, X_scaled_k5, y, cv=skf, scoring='f1_weighted')\n",
    "f1_macro_k5 = cross_val_score(knn_k5, X_scaled_k5, y, cv=skf, scoring='f1_macro')\n",
    "\n",
    "print(\"k-NN (k=5) Cross-Validation Results:\")\n",
    "print(f\"  Accuracy:      {accuracy_k5.mean():.4f} ± {accuracy_k5.std():.4f}\")\n",
    "print(f\"  F1-Weighted:   {f1_weighted_k5.mean():.4f} ± {f1_weighted_k5.std():.4f}\")\n",
    "print(f\"  F1-Macro:      {f1_macro_k5.mean():.4f} ± {f1_macro_k5.std():.4f}\")\n",
    "\n",
    "knn_k5.fit(X_scaled_k5, y)\n",
    "y_pred_k5 = cross_val_predict(knn_k5, X_scaled_k5, y, cv=skf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 k-NN with k=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN with k=7\n",
    "scaler_k7 = StandardScaler()\n",
    "X_scaled_k7 = scaler_k7.fit_transform(X)\n",
    "\n",
    "knn_k7 = KNeighborsClassifier(n_neighbors=7, metric='euclidean')\n",
    "\n",
    "accuracy_k7 = cross_val_score(knn_k7, X_scaled_k7, y, cv=skf, scoring='accuracy')\n",
    "f1_weighted_k7 = cross_val_score(knn_k7, X_scaled_k7, y, cv=skf, scoring='f1_weighted')\n",
    "f1_macro_k7 = cross_val_score(knn_k7, X_scaled_k7, y, cv=skf, scoring='f1_macro')\n",
    "\n",
    "print(\"k-NN (k=7) Cross-Validation Results:\")\n",
    "print(f\"  Accuracy:      {accuracy_k7.mean():.4f} ± {accuracy_k7.std():.4f}\")\n",
    "print(f\"  F1-Weighted:   {f1_weighted_k7.mean():.4f} ± {f1_weighted_k7.std():.4f}\")\n",
    "print(f\"  F1-Macro:      {f1_macro_k7.mean():.4f} ± {f1_macro_k7.std():.4f}\")\n",
    "\n",
    "knn_k7.fit(X_scaled_k7, y)\n",
    "y_pred_k7 = cross_val_predict(knn_k7, X_scaled_k7, y, cv=skf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 k-NN with k=9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN with k=9\n",
    "scaler_k9 = StandardScaler()\n",
    "X_scaled_k9 = scaler_k9.fit_transform(X)\n",
    "\n",
    "knn_k9 = KNeighborsClassifier(n_neighbors=9, metric='euclidean')\n",
    "\n",
    "accuracy_k9 = cross_val_score(knn_k9, X_scaled_k9, y, cv=skf, scoring='accuracy')\n",
    "f1_weighted_k9 = cross_val_score(knn_k9, X_scaled_k9, y, cv=skf, scoring='f1_weighted')\n",
    "f1_macro_k9 = cross_val_score(knn_k9, X_scaled_k9, y, cv=skf, scoring='f1_macro')\n",
    "\n",
    "print(\"k-NN (k=9) Cross-Validation Results:\")\n",
    "print(f\"  Accuracy:      {accuracy_k9.mean():.4f} ± {accuracy_k9.std():.4f}\")\n",
    "print(f\"  F1-Weighted:   {f1_weighted_k9.mean():.4f} ± {f1_weighted_k9.std():.4f}\")\n",
    "print(f\"  F1-Macro:      {f1_macro_k9.mean():.4f} ± {f1_macro_k9.std():.4f}\")\n",
    "\n",
    "knn_k9.fit(X_scaled_k9, y)\n",
    "y_pred_k9 = cross_val_predict(knn_k9, X_scaled_k9, y, cv=skf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comparison of k Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame({\n",
    "    'k': [3, 5, 7, 9],\n",
    "    'Accuracy': [accuracy_k3.mean(), accuracy_k5.mean(), accuracy_k7.mean(), accuracy_k9.mean()],\n",
    "    'Accuracy_Std': [accuracy_k3.std(), accuracy_k5.std(), accuracy_k7.std(), accuracy_k9.std()],\n",
    "    'F1_Weighted': [f1_weighted_k3.mean(), f1_weighted_k5.mean(), f1_weighted_k7.mean(), f1_weighted_k9.mean()],\n",
    "    'F1_Weighted_Std': [f1_weighted_k3.std(), f1_weighted_k5.std(), f1_weighted_k7.std(), f1_weighted_k9.std()],\n",
    "    'F1_Macro': [f1_macro_k3.mean(), f1_macro_k5.mean(), f1_macro_k7.mean(), f1_macro_k9.mean()],\n",
    "    'F1_Macro_Std': [f1_macro_k3.std(), f1_macro_k5.std(), f1_macro_k7.std(), f1_macro_k9.std()]\n",
    "})\n",
    "\n",
    "print(\"k-NN Performance Comparison:\")\n",
    "print(\"=\"*80)\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Accuracy\n",
    "axes[0].errorbar(comparison_df['k'], comparison_df['Accuracy'], \n",
    "                 yerr=comparison_df['Accuracy_Std'], \n",
    "                 marker='o', capsize=5, capthick=2, linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel('k (Number of Neighbors)', fontsize=12)\n",
    "axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[0].set_title('Accuracy vs k', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_xticks([3, 5, 7, 9])\n",
    "\n",
    "# F1-Weighted\n",
    "axes[1].errorbar(comparison_df['k'], comparison_df['F1_Weighted'], \n",
    "                 yerr=comparison_df['F1_Weighted_Std'], \n",
    "                 marker='o', capsize=5, capthick=2, linewidth=2, markersize=8, color='orange')\n",
    "axes[1].set_xlabel('k (Number of Neighbors)', fontsize=12)\n",
    "axes[1].set_ylabel('F1-Weighted', fontsize=12)\n",
    "axes[1].set_title('F1-Weighted vs k', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_xticks([3, 5, 7, 9])\n",
    "\n",
    "# F1-Macro\n",
    "axes[2].errorbar(comparison_df['k'], comparison_df['F1_Macro'], \n",
    "                 yerr=comparison_df['F1_Macro_Std'], \n",
    "                 marker='o', capsize=5, capthick=2, linewidth=2, markersize=8, color='green')\n",
    "axes[2].set_xlabel('k (Number of Neighbors)', fontsize=12)\n",
    "axes[2].set_ylabel('F1-Macro', fontsize=12)\n",
    "axes[2].set_title('F1-Macro vs k', fontsize=14, fontweight='bold')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "axes[2].set_xticks([3, 5, 7, 9])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. k-NN Analysis and Conclusions\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Best k value**: k=3 achieves the highest performance\n",
    "   - Accuracy: ~68.84%\n",
    "   - F1-Weighted: ~68.12%\n",
    "   - F1-Macro: ~54.37%\n",
    "\n",
    "2. **Performance decreases as k increases**:\n",
    "   - k=3: Most precise, uses closest neighbors\n",
    "   - k=9: Too smooth, loses detail\n",
    "\n",
    "3. **Well-predicted classes** (k=3):\n",
    "   - AAA: 89% F1-Score (226 examples)\n",
    "   - AA+: 80% F1-Score (50 examples)\n",
    "   - BB: 78% F1-Score (12 examples)\n",
    "\n",
    "4. **Difficult classes** (k=3):\n",
    "   - CC, CCC, CCC+: 0% (only 3-4 examples each)\n",
    "   - Classes with <10 examples are very hard to predict\n",
    "\n",
    "5. **Overall performance**:\n",
    "   - 68.84% accuracy with 20 classes is excellent\n",
    "   - Random guessing would give ~5% accuracy\n",
    "   - k-NN is 13× better than random!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved metrics\n",
    "metrics_df = pd.read_csv('../results/classification_metrics.csv')\n",
    "print(\"Saved Classification Metrics:\")\n",
    "print(\"=\"*80)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Naive Bayes Classification\n",
    "\n",
    "Naive Bayes is a probabilistic classifier based on Bayes' theorem. It calculates the probability of each class given the features and predicts the class with the highest probability.\n",
    "\n",
    "**Key points:**\n",
    "- No feature normalization needed (unlike k-NN)\n",
    "- Assumes features are independent (naive assumption)\n",
    "- Fast training and prediction\n",
    "- Good probabilistic baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Gaussian Naive Bayes model\n",
    "nb_model = GaussianNB()\n",
    "\n",
    "# Cross-validation scores (no normalization needed)\n",
    "accuracy_nb = cross_val_score(nb_model, X, y, cv=skf, scoring='accuracy')\n",
    "f1_weighted_nb = cross_val_score(nb_model, X, y, cv=skf, scoring='f1_weighted')\n",
    "f1_macro_nb = cross_val_score(nb_model, X, y, cv=skf, scoring='f1_macro')\n",
    "\n",
    "print(\"Naive Bayes Cross-Validation Results:\")\n",
    "print(f\"  Accuracy:      {accuracy_nb.mean():.4f} ± {accuracy_nb.std():.4f}\")\n",
    "print(f\"  F1-Weighted:   {f1_weighted_nb.mean():.4f} ± {f1_weighted_nb.std():.4f}\")\n",
    "print(f\"  F1-Macro:      {f1_macro_nb.mean():.4f} ± {f1_macro_nb.std():.4f}\")\n",
    "\n",
    "# Train final model\n",
    "nb_model.fit(X, y)\n",
    "print(\"\\n✓ Model trained on full dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix for Naive Bayes\n",
    "y_pred_nb = cross_val_predict(nb_model, X, y, cv=skf)\n",
    "cm_nb = confusion_matrix(y, y_pred_nb)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(14, 12))\n",
    "labels = sorted(y.unique())\n",
    "sns.heatmap(cm_nb, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=labels, yticklabels=labels,\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title(f'Naive Bayes Confusion Matrix\\nAccuracy: {accuracy_score(y, y_pred_nb):.4f}', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report for Naive Bayes\n",
    "print(\"Classification Report (Naive Bayes):\")\n",
    "print(classification_report(y, y_pred_nb, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparison: k-NN vs Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe including Naive Bayes\n",
    "comparison_all_df = pd.DataFrame({\n",
    "    'Model': ['k-NN (k=3)', 'k-NN (k=5)', 'k-NN (k=7)', 'k-NN (k=9)', 'Naive Bayes'],\n",
    "    'Accuracy': [accuracy_k3.mean(), accuracy_k5.mean(), accuracy_k7.mean(), accuracy_k9.mean(), accuracy_nb.mean()],\n",
    "    'Accuracy_Std': [accuracy_k3.std(), accuracy_k5.std(), accuracy_k7.std(), accuracy_k9.std(), accuracy_nb.std()],\n",
    "    'F1_Weighted': [f1_weighted_k3.mean(), f1_weighted_k5.mean(), f1_weighted_k7.mean(), f1_weighted_k9.mean(), f1_weighted_nb.mean()],\n",
    "    'F1_Weighted_Std': [f1_weighted_k3.std(), f1_weighted_k5.std(), f1_weighted_k7.std(), f1_weighted_k9.std(), f1_weighted_nb.std()],\n",
    "    'F1_Macro': [f1_macro_k3.mean(), f1_macro_k5.mean(), f1_macro_k7.mean(), f1_macro_k9.mean(), f1_macro_nb.mean()],\n",
    "    'F1_Macro_Std': [f1_macro_k3.std(), f1_macro_k5.std(), f1_macro_k7.std(), f1_macro_k9.std(), f1_macro_nb.std()]\n",
    "})\n",
    "\n",
    "print(\"All Models Performance Comparison:\")\n",
    "print(\"=\"*80)\n",
    "comparison_all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "models = ['k-NN\\n(k=3)', 'k-NN\\n(k=5)', 'k-NN\\n(k=7)', 'k-NN\\n(k=9)', 'Naive\\nBayes']\n",
    "x_pos = np.arange(len(models))\n",
    "\n",
    "# Accuracy\n",
    "axes[0].bar(x_pos, comparison_all_df['Accuracy'], yerr=comparison_all_df['Accuracy_Std'], \n",
    "            capsize=5, alpha=0.7, color=['#1f77b4', '#1f77b4', '#1f77b4', '#1f77b4', '#ff7f0e'])\n",
    "axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[0].set_title('Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xticks(x_pos)\n",
    "axes[0].set_xticklabels(models)\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# F1-Weighted\n",
    "axes[1].bar(x_pos, comparison_all_df['F1_Weighted'], yerr=comparison_all_df['F1_Weighted_Std'], \n",
    "            capsize=5, alpha=0.7, color=['#1f77b4', '#1f77b4', '#1f77b4', '#1f77b4', '#ff7f0e'])\n",
    "axes[1].set_ylabel('F1-Weighted', fontsize=12)\n",
    "axes[1].set_title('F1-Weighted Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xticks(x_pos)\n",
    "axes[1].set_xticklabels(models)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# F1-Macro\n",
    "axes[2].bar(x_pos, comparison_all_df['F1_Macro'], yerr=comparison_all_df['F1_Macro_Std'], \n",
    "            capsize=5, alpha=0.7, color=['#1f77b4', '#1f77b4', '#1f77b4', '#1f77b4', '#ff7f0e'])\n",
    "axes[2].set_ylabel('F1-Macro', fontsize=12)\n",
    "axes[2].set_title('F1-Macro Comparison', fontsize=14, fontweight='bold')\n",
    "axes[2].set_xticks(x_pos)\n",
    "axes[2].set_xticklabels(models)\n",
    "axes[2].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Overall Analysis\n",
    "\n",
    "### Naive Bayes Performance:\n",
    "\n",
    "**Results:**\n",
    "- Accuracy: ~37.26%\n",
    "- F1-Weighted: ~37.32%\n",
    "- F1-Macro: ~30.67%\n",
    "\n",
    "**Why is Naive Bayes much weaker than k-NN?**\n",
    "\n",
    "1. **Independence assumption violated**: Naive Bayes assumes features are independent, but in reality:\n",
    "   - Interest_Rate and Inflation are correlated\n",
    "   - Unemployment and GDP_Growth are correlated\n",
    "   - This violates the \"naive\" assumption\n",
    "\n",
    "2. **20 classes with imbalanced data**: \n",
    "   - Classes with few examples (CC=3, CCC=4) are very difficult\n",
    "   - Gaussian assumption may not hold for all features\n",
    "\n",
    "3. **k-NN handles correlation naturally**: \n",
    "   - k-NN uses distances, not probabilities\n",
    "   - No independence assumption needed\n",
    "\n",
    "### Best Model So Far:\n",
    "\n",
    "**k-NN (k=3)** is the clear winner:\n",
    "- 68.84% accuracy (vs 37.26% for Naive Bayes)\n",
    "- 85% better than Naive Bayes\n",
    "- 13× better than random guessing (5%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved metrics\n",
    "metrics_df = pd.read_csv('../results/classification_metrics.csv')\n",
    "print(\"Saved Classification Metrics:\")\n",
    "print(\"=\"*80)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "In the following sections, we will implement and compare:\n",
    "- Decision Tree (interpretable model with max_depth tuning)\n",
    "- Random Forest (ensemble method for improved performance)\n",
    "\n",
    "These tree-based models should handle feature correlation better than Naive Bayes and potentially match or exceed k-NN performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
