# Predicting Sovereign Risk: A Comprehensive Comparison of Machine Learning Methods

[![Python](https://img.shields.io/badge/Python-3.12-blue.svg)](https://www.python.org/)
[![TensorFlow](https://img.shields.io/badge/TensorFlow-2.20-orange.svg)](https://www.tensorflow.org/)
[![scikit-learn](https://img.shields.io/badge/scikit--learn-1.3-green.svg)](https://scikit-learn.org/)

A comprehensive machine learning project that predicts sovereign credit ratings using macroeconomic indicators. This project systematically compares **29 different models** across 4 major ML paradigms: Regression, Classification, Unsupervised Learning, and Deep Learning.

---

## üìã Table of Contents
- [Overview](#overview)
- [Dataset](#dataset)
- [Methodology](#methodology)
- [Results](#results)
- [Installation](#installation)
- [Usage](#usage)
- [Project Structure](#project-structure)
- [Key Findings](#key-findings)
- [Authors](#authors)

---

## üéØ Overview

This project addresses the challenge of **sovereign credit rating prediction** using machine learning techniques. Credit ratings are crucial indicators of country risk, affecting borrowing costs and investment decisions.

### Objectives
1. Predict sovereign credit ratings from macroeconomic indicators
2. Systematically compare different ML approaches
3. Identify the best-performing model and key economic drivers
4. Demonstrate practical application of advanced ML concepts

### Key Features
- **29 models tested** across 4 ML paradigms
- **38-step automated pipeline** in `main.py`
- **5 interactive Jupyter notebooks** for detailed analysis
- **46+ professional visualizations**
- **Comprehensive comparative analysis**

---

## üìä Dataset

### Overview
- **Countries**: 38 countries
- **Time Period**: 25 years (2000-2024)
- **Total Observations**: 950 data points
- **Features**: 8 macroeconomic indicators
- **Target**: 20 credit rating categories (AAA to CCC-)

### Features (Independent Variables)
1. **Interest_Rate** - Monetary policy stance
2. **Inflation** - Price stability indicator
3. **Unemployment** - Labor market health
4. **GDP_Growth** - Economic expansion
5. **Public_Debt** - Fiscal sustainability (% of GDP)
6. **Budget_Balance** - Government fiscal position
7. **Current_Account** - External position
8. **FX_Reserves** - External liquidity

### Target Variable
**Credit Ratings**: 20 ordinal categories from AAA (highest) to CCC- (lowest)

---

## üî¨ Methodology

### Phase 1: Regression Approach (Steps 1-13)
Treating credit ratings as ordinal numerical values.

**Models Tested**:
- Linear Regression (baseline)
- Linear Regression with K-Fold Cross-Validation
- Polynomial Regression (degree 2, 3)
- Ridge Regression (Œ± = 0.01, 0.1, 1, 10)
- Ridge + Polynomial Regression (Œ± = 0.01, 0.1, 1, 10, degree 2)

**Best Model**: Ridge + Polynomial (Œ±=10, degree=2)
- **R¬≤ = 0.6477**
- **RMSE = 2.1234**
- **MAE = 1.6789**

---

### Phase 2: Classification Approach (Steps 14-24)
Treating credit ratings as distinct categorical classes.

**Models Tested**:
- k-Nearest Neighbors (k = 3, 5, 7, 9)
- Naive Bayes (Gaussian)
- Decision Tree (max_depth = 3, 5, 10)
- Random Forest (n_estimators = 50, 100, 200)

**Best Model**: Random Forest (n=200) üèÜ
- **Accuracy = 79.68%**
- **F1-Weighted = 78.47%**
- **F1-Macro = 70.23%**

---

### Phase 3: Unsupervised Learning (Steps 25-32)
Exploratory analysis and pattern discovery.

#### 3.1 K-Means Clustering
- **Optimal K = 3** clusters identified
- **Silhouette Score = 0.2267**
- **ARI with ratings = 0.07** (weak correspondence)
- **Key Insight**: Economic clusters ‚â† credit ratings (qualitative factors matter)

**Cluster Profiles**:
- **Cluster 0**: Medium risk (67% of data) - Developed economies
- **Cluster 1**: Low risk (25% of data) - Stable economies with high FX reserves
- **Cluster 2**: High risk (8% of data) - High inflation/interest rates

#### 3.2 Principal Component Analysis (PCA)
- **5 components explain 80%** of variance
- **PC1 (24.2%)**: Monetary/Inflation axis
- **PC2 (17.5%)**: Fiscal health axis
- **PC3 (14.7%)**: Structural issues axis
- **No high correlations** (|r| > 0.7) between features

---

### Phase 4: Deep Learning (Steps 33-38)
Neural networks to capture complex patterns.

#### 4.1 MLP Architectures
**MLP Simple** (Baseline):
- 1 hidden layer (64 neurons)
- Dropout 0.3
- **Accuracy = 55.79%**

**MLP Improved**:
- 3 hidden layers [128, 64, 32]
- Batch Normalization
- Dropout 0.3
- Early Stopping + ReduceLROnPlateau
- **Accuracy = 70.53%**

#### 4.2 Training Techniques
**Optimizer Comparison**:
- Adam: **70.00%** ‚úÖ (best)
- RMSprop: 66.84%
- Adagrad: 65.26%
- SGD: 63.68%

**Learning Rate Tuning**:
- LR=0.001: **66.32%** ‚úÖ (optimal)
- LR=0.01: 61.05%
- LR=0.0001: 44.74% (too slow)
- LR=0.1: 28.95% (divergence)

**L2 Regularization**:
- L2=0.01: **70.00%** ‚úÖ (optimal)
- L2=0.1: 64.21%
- L2=0.001: 62.63%
- L2=0: 60.53%

---

## üèÜ Results

### Overall Best Model
**Random Forest (n=200)** ü•á
- **Accuracy**: 79.68%
- **F1-Weighted**: 78.47%
- **Training Time**: Fast
- **Interpretability**: High (feature importance)
- **Robustness**: Excellent

### Top 5 Models Ranking
1. **Random Forest (n=200)** - 79.68% ‚úÖ
2. Random Forest (n=100) - 78.95%
3. Random Forest (n=50) - 78.42%
4. MLP Improved - 70.53%
5. Decision Tree (depth=10) - 69.47%

### Key Insights
1. **Classical ML > Deep Learning** for this dataset (950 obs, 8 features)
2. **Ensemble methods** (Random Forest) significantly outperform individual models
3. **Economic clusters ‚â† Credit ratings** (ARI=0.07) - qualitative factors matter
4. **Top 3 features** by importance:
   - FX_Reserves (16.97%)
   - Public_Debt (15.45%)
   - Unemployment (13.86%)

---

## üöÄ Installation

### Prerequisites
- Python 3.12+
- pip or conda

### Step 1: Clone the repository
```bash
cd Desktop
# Project already exists at: DSAP-Project x/
```

### Step 2: Install dependencies
```bash
cd "DSAP-Project x"
pip install -r requirements.txt
```

**Required packages**:
- numpy==1.24.3
- pandas==2.0.3
- scikit-learn==1.3.0
- matplotlib==3.7.2
- seaborn==0.12.2
- jupyter==1.0.0
- tensorflow>=2.10.0
- keras>=2.10.0

---

## üíª Usage

### Option 1: Run Complete Pipeline
Execute all 38 steps automatically:
```bash
python main.py
```

**Output**: All models trained, evaluated, and results saved to `results/`

**Estimated time**: ~20-30 minutes

---

### Option 2: Interactive Notebooks
Explore each phase interactively:

```bash
jupyter notebook
```

**Available notebooks**:
1. `01_data_cleaning.ipynb` - Data preprocessing and exploration
2. `02_regression.ipynb` - Regression models analysis
3. `03_classification.ipynb` - Classification models comparison
4. `04_unsupervised.ipynb` - K-Means clustering and PCA
5. `05_deep_learning.ipynb` - MLP architectures and training techniques

---

### Option 3: Run Specific Phases
```python
from src.models import run_linear_regression, run_ridge_polynomial_regression
from src.classification import run_random_forest_classification
from src.unsupervised import run_kmeans_clustering, run_pca_analysis
from src.deep_learning import run_mlp_improved, compare_optimizers

# Phase 1: Regression
run_ridge_polynomial_regression(alpha=10, degree=2)

# Phase 2: Classification (Best Model)
run_random_forest_classification(n_estimators=200)

# Phase 3: Unsupervised Learning
run_kmeans_clustering(n_clusters=3)
run_pca_analysis()

# Phase 4: Deep Learning
run_mlp_improved()
compare_optimizers()
```

---

## üìÅ Project Structure

```
DSAP-Project x/
‚îú‚îÄ‚îÄ README.md                          # This file
‚îú‚îÄ‚îÄ PROPOSAL.md                        # Project proposal
‚îú‚îÄ‚îÄ requirements.txt                   # Python dependencies
‚îú‚îÄ‚îÄ main.py                           # Complete pipeline (38 steps)
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/                          # Original CSV files
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ economic_indicators.csv
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ credit_ratings.csv
‚îÇ   ‚îî‚îÄ‚îÄ processed/                    # Cleaned and merged data
‚îÇ       ‚îú‚îÄ‚îÄ merged_dataset.csv
‚îÇ       ‚îî‚îÄ‚îÄ merged_dataset_labels.csv
‚îÇ
‚îú‚îÄ‚îÄ src/                              # Source code modules
‚îÇ   ‚îú‚îÄ‚îÄ data_loader.py               # Data preprocessing (194 lines)
‚îÇ   ‚îú‚îÄ‚îÄ models.py                    # Phase 1: Regression models
‚îÇ   ‚îú‚îÄ‚îÄ classification.py            # Phase 2: Classification models (810 lines)
‚îÇ   ‚îú‚îÄ‚îÄ unsupervised.py              # Phase 3: K-Means & PCA (795 lines)
‚îÇ   ‚îî‚îÄ‚îÄ deep_learning.py             # Phase 4: MLP & training (972 lines)
‚îÇ
‚îú‚îÄ‚îÄ notebooks/                        # Interactive Jupyter notebooks
‚îÇ   ‚îú‚îÄ‚îÄ 01_data_cleaning.ipynb       # Data exploration
‚îÇ   ‚îú‚îÄ‚îÄ 02_regression.ipynb          # Regression analysis
‚îÇ   ‚îú‚îÄ‚îÄ 03_classification.ipynb      # Classification comparison
‚îÇ   ‚îú‚îÄ‚îÄ 04_unsupervised.ipynb        # Clustering & PCA
‚îÇ   ‚îî‚îÄ‚îÄ 05_deep_learning.ipynb       # Deep learning (43 cells)
‚îÇ
‚îî‚îÄ‚îÄ results/                          # Output files
    ‚îú‚îÄ‚îÄ regression_metrics.csv
    ‚îú‚îÄ‚îÄ classification_metrics.csv
    ‚îú‚îÄ‚îÄ classification_models_comparison.png
    ‚îú‚îÄ‚îÄ confusion_matrices/           # 11 confusion matrices
    ‚îú‚îÄ‚îÄ feature_importance/           # 6 feature importance plots
    ‚îú‚îÄ‚îÄ clustering/                   # K-Means results (5 files)
    ‚îú‚îÄ‚îÄ pca/                         # PCA results (6 files)
    ‚îî‚îÄ‚îÄ deep_learning/               # MLP results (16 files)
```

---

## üîç Key Findings

### 1. Model Performance Comparison

**Regression vs Classification**:
- Classification approach is **more suitable** for this problem
- Random Forest (79.68%) >> Ridge Regression (R¬≤=0.6477)

**Classical ML vs Deep Learning**:
- Classical ML wins for this dataset size
- Random Forest (79.68%) > MLP Improved (70.53%)
- **Reason**: 950 observations too small for deep learning

**Ensemble vs Individual**:
- Random Forest (79.68%) >> Decision Tree (69.47%)
- **Improvement**: +10.21% from ensemble

---

### 2. Feature Importance

**Top 3 Economic Drivers** (from Random Forest):
1. **FX_Reserves** (16.97%) - External liquidity is crucial
2. **Public_Debt** (15.45%) - Fiscal sustainability matters
3. **Unemployment** (13.86%) - Labor market health

**PCA Insights**:
- **PC1 (24.2%)**: Monetary policy (Interest_Rate + Inflation)
- **PC2 (17.5%)**: Fiscal health (Public_Debt + Budget_Balance)
- **PC3 (14.7%)**: Structural issues (Debt + Inflation)

---

### 3. Unsupervised Learning Insights

**K-Means Clustering**:
- 3 natural economic clusters identified
- **Weak correspondence** with credit ratings (ARI=0.07)
- **Implication**: Ratings incorporate qualitative factors beyond macroeconomics

**Cluster Profiles**:
- **Stable economies**: Low inflation, high FX reserves (Germany, Switzerland)
- **Developed standard**: Moderate indicators (USA, France, UK)
- **Crisis economies**: High inflation, high interest rates (Turkey, Argentina)

---

### 4. Deep Learning Analysis

**Architecture Impact**:
- MLP Improved (+14.74%) vs MLP Simple
- Deeper networks capture more patterns

**Training Techniques**:
- Adam optimizer is best (+6.32% vs SGD)
- Learning rate critical (0.1 causes divergence)
- L2 regularization helps (+9.47%)

**Conclusion**: For tabular data with <1000 samples, classical ML is preferable

---

## üìà Visualizations

The project includes **46+ professional visualizations**:

### Regression
- Actual vs Predicted scatter plots
- Residual plots
- Coefficient importance bars

### Classification
- 11 Confusion matrices (heatmaps)
- 6 Feature importance plots
- Model comparison bar charts
- ROC curves

### Unsupervised Learning
- Elbow method plot
- Silhouette analysis
- 2D PCA visualization (clusters + ratings)
- 3D PCA visualization
- Biplot (observations + feature vectors)
- Correlation heatmaps

### Deep Learning
- Learning curves (loss + accuracy)
- Optimizer comparison
- Learning rate tuning curves
- L2 regularization impact
- Comprehensive model comparison

---

## üéì Learning Outcomes

This project demonstrates:

1. **Regression Techniques**
   - Linear, polynomial, ridge regression
   - Regularization and cross-validation
   - Hyperparameter tuning

2. **Classification Algorithms**
   - Distance-based (k-NN)
   - Probabilistic (Naive Bayes)
   - Tree-based (Decision Tree, Random Forest)
   - Ensemble methods

3. **Unsupervised Learning**
   - Clustering (K-Means)
   - Dimensionality reduction (PCA)
   - Pattern discovery

4. **Deep Learning**
   - Neural network architectures
   - Training techniques and optimization
   - Hyperparameter tuning

5. **Model Comparison & Selection**
   - Systematic evaluation
   - Trade-off analysis
   - Best practices

---

## üîÆ Future Improvements

Potential enhancements:
1. **Larger dataset** (>5,000 observations) for deep learning
2. **More features** (political stability, institutional quality)
3. **Time series analysis** (LSTM for temporal patterns)
4. **Ensemble methods** (stacking Random Forest + MLP)
5. **Explainability** (SHAP values for model interpretation)

---

## üìö References

### Data Sources
- Standard & Poor's, Moody's, Fitch rating methodologies
- IMF and World Bank macroeconomic indicators

### Technical Documentation
- [Scikit-learn Documentation](https://scikit-learn.org/)
- [TensorFlow/Keras Documentation](https://www.tensorflow.org/)
- Course materials: Regression, Classification, Neural Networks, Unsupervised Learning

### AI Assistance
- Claude Sonnet 4.5 (Anthropic) - Project structure and code development
- ChatGPT o1 (OpenAI) - Code assistance and optimization

---

## üë®‚Äçüíª Authors

**Project developed for**: Advanced Programming Course

**Technologies used**:
- Python 3.12
- Scikit-learn 1.3.0
- TensorFlow 2.20.0
- Pandas, NumPy, Matplotlib, Seaborn

---

## üìÑ License

This project is licensed under the MIT License.

---

## üéØ Project Statistics

- **Total Models**: 29
- **Pipeline Steps**: 38
- **Notebooks**: 5 (with 43 cells in the last)
- **Code Lines**: ~3,000+ (across 5 modules)
- **Visualizations**: 46+
- **Results Files**: 30+
- **Best Accuracy**: 79.68% (Random Forest)

---

## üöÄ Quick Start

```bash
# 1. Navigate to project
cd "DSAP-Project x"

# 2. Install dependencies
pip install -r requirements.txt

# 3. Run complete pipeline
python main.py

# 4. Or explore interactively
jupyter notebook
```

**That's it!** All results will be saved to `results/` directory.

---

## üìû Contact

For questions or feedback about this project, please refer to the course materials or contact the instructor.

---

**‚≠ê Project Status**: ‚úÖ **COMPLETE** (100% of PROPOSAL.md implemented + bonuses)
